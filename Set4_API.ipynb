{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json\")\n",
    "status_code = response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"lat\": 40.71, \"lon\": -74}\n",
    "\n",
    "# Make a get request with the parameters.\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json\", params=parameters)\n",
    "\n",
    "# Print the content of the response (the data the server returned)\n",
    "print(response.content)\n",
    "\n",
    "# This gets the same data as the command above\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json?lat=40.71&lon=-74\")\n",
    "print(response.content)\n",
    "parameters = {\"lat\": 37.78, \"lon\": -122.41}\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json\", params=parameters)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of fast food chains.\n",
    "best_food_chains = [\"Taco Bell\", \"Shake Shack\", \"Chipotle\"]\n",
    "print(type(best_food_chains))\n",
    "\n",
    "# Import the JSON library.\n",
    "import json\n",
    "\n",
    "# Use json.dumps to convert best_food_chains to a string.\n",
    "best_food_chains_string = json.dumps(best_food_chains)\n",
    "print(type(best_food_chains_string))\n",
    "\n",
    "# Convert best_food_chains_string back to a list.\n",
    "print(type(json.loads(best_food_chains_string)))\n",
    "\n",
    "# Make a dictionary\n",
    "fast_food_franchise = {\n",
    "    \"Subway\": 24722,\n",
    "    \"McDonalds\": 14098,\n",
    "    \"Starbucks\": 10821,\n",
    "    \"Pizza Hut\": 7600\n",
    "}\n",
    "\n",
    "# We can also dump a dictionary to a string and load it.\n",
    "fast_food_franchise_string = json.dumps(fast_food_franchise)\n",
    "print(type(fast_food_franchise_string))\n",
    "fast_food_franchise_2 = json.loads(fast_food_franchise_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the same request we did two screens ago.\n",
    "parameters = {\"lat\": 37.78, \"lon\": -122.41}\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json\", params=parameters)\n",
    "\n",
    "# Get the response data as a Python object.  Verify that it's a dictionary.\n",
    "json_data = response.json()\n",
    "print(type(json_data))\n",
    "print(json_data)\n",
    "first_pass_duration = json_data[\"response\"][0][\"duration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.headers)\n",
    "content_type = response.headers[\"content-type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the API here.\n",
    "response = requests.get(\"http://api.open-notify.org/astros.json\")\n",
    "json_data = response.json()\n",
    "\n",
    "in_space_count = json_data[\"number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of headers containing our Authorization header.\n",
    "headers = {\"Authorization\": \"token 1f36137fbbe1602f779300dad26e4c1b7fbab631\"}\n",
    "\n",
    "# Make a GET request to the GitHub API with our headers.\n",
    "# This API endpoint will give us details about Vik Paruchuri.\n",
    "response = requests.get(\"https://api.github.com/users/VikParuchuri\", headers=headers)\n",
    "\n",
    "# Print the content of the response.  As you can see, this token corresponds to the account of Vik Paruchuri.\n",
    "print(response.json())\n",
    "response = requests.get(\"https://api.github.com/users/VikParuchuri/orgs\", headers=headers)\n",
    "orgs = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've loaded headers in.\n",
    "response = requests.get(\"https://api.github.com/users/torvalds\", headers=headers)\n",
    "torvalds = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://api.github.com/repos/octocat/Hello-World\", headers=headers)\n",
    "hello_world = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"per_page\": 50, \"page\": 1}\n",
    "response = requests.get(\"https://api.github.com/users/VikParuchuri/starred\", headers=headers, params=params)\n",
    "page1_repos = response.json()\n",
    "params = {\"per_page\": 50, \"page\": 2}\n",
    "response = requests.get(\"https://api.github.com/users/VikParuchuri/starred\", headers=headers, params=params)\n",
    "page2_repos = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://api.github.com/user\", headers=headers)\n",
    "user = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data we'll pass into the API endpoint.  While this endpoint only requires the \"name\" key, \n",
    "#there are other optional keys.\n",
    "payload = {\"name\": \"test\"}\n",
    "\n",
    "# We need to pass in our authentication headers!\n",
    "response = requests.post(\"https://api.github.com/user/repos\", json=payload, headers=headers)\n",
    "print(response.status_code)\n",
    "payload = {\"name\": \"learning-about-apis\"}\n",
    "response = requests.post(\"https://api.github.com/user/repos\", json=payload, headers=headers)\n",
    "status = response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"description\": \"The best repository ever!\", \"name\": \"test\"}\n",
    "response = requests.patch(\"https://api.github.com/repos/VikParuchuri/test\", json=payload, headers=headers)\n",
    "print(response.status_code)\n",
    "payload = {\"description\": \"Learning about requests!\", \"name\": \"learning-about-apis\"}\n",
    "response = requests.patch(\"https://api.github.com/repos/VikParuchuri/learning-about-apis\", json=payload, headers=headers)\n",
    "status = response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.delete(\"https://api.github.com/repos/VikParuchuri/test\", headers=headers)\n",
    "print(response.status_code)\n",
    "response = requests.delete(\"https://api.github.com/repos/VikParuchuri/learning-about-apis\", headers=headers)\n",
    "status = response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "headers = {\"Authorization\": \"bearer 13426216-4U1ckno9J5AiK72VRbpEeBaMSKk\", \"User-Agent\": \"Dataquest/1.0\"}\n",
    "params = {\"t\": \"day\"}\n",
    "response = requests.get(\"https://oauth.reddit.com/r/python/top\", headers=headers, params=params)\n",
    "\n",
    "python_top = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_top_articles = python_top[\"data\"][\"children\"]\n",
    "most_upvoted = \"\"\n",
    "most_upvotes = 0\n",
    "for article in python_top_articles:\n",
    "    ar = article[\"data\"]\n",
    "    if ar[\"ups\"] >= most_upvotes:\n",
    "        most_upvoted = ar[\"id\"]\n",
    "        most_upvotes = ar[\"ups\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": \"bearer 13426216-4U1ckno9J5AiK72VRbpEeBaMSKk\", \"User-Agent\": \"Dataquest/1.0\"}\n",
    "response = requests.get(\"https://oauth.reddit.com/r/python/comments/4b7w9u\", headers=headers)\n",
    "\n",
    "comments = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_list = comments[1][\"data\"][\"children\"]\n",
    "most_upvoted_comment = \"\"\n",
    "most_upvotes_comment = 0\n",
    "for comment in comments_list:\n",
    "    co = comment[\"data\"]\n",
    "    if co[\"ups\"] >= most_upvotes_comment:\n",
    "        most_upvoted_comment = co[\"id\"]\n",
    "        most_upvotes_comment = co[\"ups\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"dir\": 1, \"id\": \"d16y4ry\"}\n",
    "headers = {\"Authorization\": \"bearer 13426216-4U1ckno9J5AiK72VRbpEeBaMSKk\", \"User-Agent\": \"Dataquest/1.0\"}\n",
    "response = requests.post(\"https://oauth.reddit.com/api/vote\", json=payload, headers=headers)\n",
    "status = response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple.html\")\n",
    "content = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize the parser, and pass in the content we grabbed earlier.\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Get the body tag from the document.\n",
    "# Since we passed in the top level of the document to the parser, we need to pick a branch off of the root.\n",
    "# With BeautifulSoup, we can access branches by using tag types as attributes.\n",
    "body = parser.body\n",
    "\n",
    "# Get the p tag from the body.\n",
    "p = body.p\n",
    "\n",
    "# Print the text inside the p tag.\n",
    "# Text is a property that gets the inside text of a tag.\n",
    "print(p.text)\n",
    "head = parser.head\n",
    "title = head.title\n",
    "title_text = title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Get a list of all occurrences of the body tag in the element.\n",
    "body = parser.find_all(\"body\")\n",
    "\n",
    "# Get the paragraph tag.\n",
    "p = body[0].find_all(\"p\")\n",
    "\n",
    "# Get the text.\n",
    "print(p[0].text)\n",
    "head = parser.find_all(\"head\")\n",
    "title = head[0].find_all(\"title\")\n",
    "title_text = title[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the page content and set up a new parser.\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple_ids.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Pass in the ID attribute to only get the element with that specific ID.\n",
    "first_paragraph = parser.find_all(\"p\", id=\"first\")[0]\n",
    "print(first_paragraph.text)\n",
    "second_paragraph = parser.find_all(\"p\", id=\"second\")[0]\n",
    "second_paragraph_text = second_paragraph.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the website that contains classes.\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple_classes.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Get the first inner paragraph.\n",
    "# Find all the paragraph tags with the class inner-text.\n",
    "# Then, take the first element in that list.\n",
    "first_inner_paragraph = parser.find_all(\"p\", class_=\"inner-text\")[0]\n",
    "print(first_inner_paragraph.text)\n",
    "second_inner_paragraph = parser.find_all(\"p\", class_=\"inner-text\")[1]\n",
    "second_inner_paragraph_text = second_inner_paragraph.text\n",
    "\n",
    "first_outer_paragraph = parser.find_all(\"p\", class_=\"outer-text\")[0]\n",
    "first_outer_paragraph_text = first_outer_paragraph.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the website that contains classes and IDs.\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Select all of the elements that have the first-item class.\n",
    "first_items = parser.select(\".first-item\")\n",
    "\n",
    "# Print the text of the first paragraph (the first element with the first-item class).\n",
    "print(first_items[0].text)\n",
    "first_outer_text = parser.select(\".outer-text\")[0].text\n",
    "\n",
    "second_text = parser.select(\"#second\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Superbowl box score data.\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/2014_super_bowl.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Find the number of turnovers the Seahawks committed.\n",
    "turnovers = parser.select(\"#turnovers\")[0]\n",
    "seahawks_turnovers = turnovers.select(\"td\")[1]\n",
    "seahawks_turnovers_count = seahawks_turnovers.text\n",
    "print(seahawks_turnovers_count)\n",
    "patriots_total_plays_count = parser.select(\"#total-plays\")[0].select(\"td\")[2].text\n",
    "\n",
    "seahawks_total_yards_count = parser.select(\"#total-yards\")[0].select(\"td\")[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

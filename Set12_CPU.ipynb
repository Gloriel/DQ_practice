{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"ecommerce5000.csv\", encoding=\"Latin-1\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iz_operations = 0\n",
    "item_operations = 0\n",
    "duplicates_init = 0\n",
    "duplicates_false = 0\n",
    "duplicates_true = 0\n",
    "if_duplicate = 0\n",
    "duplicates_append = 0\n",
    "duplicates_init += 1\n",
    "# Initialize a list to store our duplicates\n",
    "duplicates = []\n",
    "query = data['query']\n",
    "# Loop through each item in the query column.\n",
    "for i, item in enumerate(query):\n",
    "    duplicates_false += 1\n",
    "    duplicate = False\n",
    "    \n",
    "    # Loop through each item in the query column.\n",
    "    for z, item2 in enumerate(query):\n",
    "        # If the outer and inner loops are on the same value, keep going.\n",
    "        # Without this, we'll falsely detect rows as duplicates.\n",
    "        iz_operations += 1\n",
    "        if i == z:\n",
    "            continue\n",
    "        # Mark as duplicate if we find a match.\n",
    "        item_operations += 1\n",
    "        if item == item2:\n",
    "            duplicates_true += 1\n",
    "            duplicate = True\n",
    "    if_duplicate += 1\n",
    "    # Add to the duplicates list.\n",
    "    if duplicate:\n",
    "        duplicates_append += 1\n",
    "        duplicates.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_increments = 0\n",
    "total = 0\n",
    "for item in query:\n",
    "    total += len(item)\n",
    "    sum_increments += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_increments = 0\n",
    "value_checks = 0\n",
    "counts = {}\n",
    "for item in query:\n",
    "    if item not in counts:\n",
    "        counts[item] = 0\n",
    "    counts_increments += 1\n",
    "    counts[item] += 1\n",
    "\n",
    "duplicates = []\n",
    "for key, val in counts.items():\n",
    "    value_checks += 1\n",
    "    if val > 1:\n",
    "        duplicates.append(key)\n",
    "\n",
    "print(counts_increments)\n",
    "print(value_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "query_series = query\n",
    "duplicate_series = query_series.duplicated()\n",
    "duplicate_values_series = query_series[duplicate_series]\n",
    "\n",
    "pandas_elapsed = time.time() - start\n",
    "print(pandas_elapsed)\n",
    "\n",
    "start = time.time()\n",
    "counts = {}\n",
    "for item in query:\n",
    "    if item not in counts:\n",
    "        counts[item] = 0\n",
    "    counts[item] += 1\n",
    "\n",
    "duplicates = []\n",
    "for key, val in counts.items():\n",
    "    if val > 1:\n",
    "        duplicates.append(key)\n",
    "elapsed = time.time() - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "def pandas_algo():\n",
    "    duplicate_series = query_series.duplicated()\n",
    "    duplicate_values_series = query_series[duplicate_series]\n",
    "    \n",
    "def algo():\n",
    "    counts = {}\n",
    "    for item in query:\n",
    "        if item not in counts:\n",
    "            counts[item] = 0\n",
    "        counts[item] += 1\n",
    "\n",
    "    duplicates = []\n",
    "    for key, val in counts.items():\n",
    "        if val > 1:\n",
    "            duplicates.append(key)\n",
    "        \n",
    "pandas_elapsed = []\n",
    "for i in range(1000):\n",
    "    start = time.time()\n",
    "    pandas_algo()\n",
    "    pandas_elapsed.append(time.time() - start)\n",
    "\n",
    "elapsed = []\n",
    "for i in range(1000):\n",
    "    start = time.time()\n",
    "    algo()\n",
    "    elapsed.append(time.time() - start)\n",
    "\n",
    "print(statistics.median(pandas_elapsed))\n",
    "print(statistics.median(elapsed))\n",
    "\n",
    "plt.hist(pandas_elapsed)\n",
    "plt.show()\n",
    "plt.hist(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "def algo():\n",
    "    unique = set()\n",
    "    duplicates = set()\n",
    "    for item in query:\n",
    "        if item in unique:\n",
    "            duplicates.add(item)\n",
    "        else:\n",
    "            unique.add(item)\n",
    "\n",
    "elapsed = []\n",
    "for i in range(1000):\n",
    "    start = time.time()\n",
    "    algo()\n",
    "    elapsed.append(time.time() - start)\n",
    "\n",
    "print(statistics.median(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo():\n",
    "    unique = set()\n",
    "    duplicates = set()\n",
    "    for item in query:\n",
    "        if item in unique:\n",
    "            duplicates.add(item)\n",
    "        else:\n",
    "            unique.add(item)\n",
    "import cProfile\n",
    "cProfile.run(\"algo()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import statistics\n",
    "def run_with_timing(func):\n",
    "    elapsed = []\n",
    "    for i in range(10):\n",
    "        start = time.time()\n",
    "        func()\n",
    "        elapsed.append(time.time() - start)\n",
    "    return statistics.median(elapsed)\n",
    "\n",
    "def pandas_algo():\n",
    "    get_max_relevance = lambda x: x.loc[x[\"relevance\"].idxmax(), \"product_link\"]\n",
    "    return data.groupby(\"query\").apply(get_max_relevance)\n",
    "\n",
    "def algo():\n",
    "    links = {}\n",
    "    for i, row in enumerate(query):\n",
    "        if row not in links:\n",
    "            links[row] = [0,\"\"]\n",
    "        if relevance[i] > links[row][0]:\n",
    "            links[row] = [relevance[i], product_link[i]]\n",
    "    return links\n",
    "\n",
    "pandas_elapsed = run_with_timing(pandas_algo)\n",
    "elapsed = run_with_timing(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import statistics\n",
    "\n",
    "algo1_time_complexity = 0\n",
    "algo1_space_complexity = 0\n",
    "\n",
    "def algo1(data):\n",
    "    total = 0\n",
    "    for index, row in data.iterrows():\n",
    "        total += int(row[\"rank\"])\n",
    "    return total\n",
    "\n",
    "algo2_time_complexity = 0\n",
    "algo2_space_complexity = 0\n",
    "\n",
    "def algo2(data):\n",
    "    prices = []\n",
    "    for index, row in data.iterrows():\n",
    "        price_search = re.search('.*(\\d+).*', row[\"product_price\"], re.IGNORECASE)\n",
    "\n",
    "        if price_search:\n",
    "            price = float(price_search.group(1))\n",
    "        else:\n",
    "            price = None\n",
    "        prices.append(price)\n",
    "    price_avg = statistics.mean([p for p in prices if p is not None])\n",
    "    weighted_relevance = []\n",
    "    for index, row in data.iterrows():\n",
    "        if prices[index] is not None:\n",
    "            price = prices[index] / price_avg\n",
    "        else:\n",
    "            price = price_avg\n",
    "        weighted_relevance.append(float(row[\"relevance\"]) * price)\n",
    "    return weighted_relevance\n",
    "algo1_time_complexity = 1\n",
    "algo1_space_complexity = 0\n",
    "\n",
    "algo2_time_complexity = 1\n",
    "algo2_space_complexity = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import sqlite3\n",
    "\n",
    "query = \"SELECT DISTINCT teamID from Teams inner join TeamsFranchises on Teams.franchID == TeamsFranchises.franchID where TeamsFranchises.active = 'Y';\"\n",
    "conn = sqlite3.connect(\"lahman2015.sqlite\")\n",
    "\n",
    "cur = conn.cursor()\n",
    "teams = [row[0] for row in cur.execute(query).fetchall()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "memory = sqlite3.connect(':memory:') # create a memory database\n",
    "disk = sqlite3.connect('lahman2015.sqlite')\n",
    "\n",
    "dump = \"\".join([line for line in disk.iterdump() if \"Batting\" in line])\n",
    "memory.executescript(dump)\n",
    "\n",
    "cur = memory.cursor()\n",
    "query = \"SELECT SUM(HR) FROM Batting WHERE teamId=?\"\n",
    "def calculate_runs(teams):\n",
    "    home_runs = []\n",
    "    for team in teams:\n",
    "        runs = cur.execute(query, [team]).fetchall()\n",
    "        runs = runs[0][0]\n",
    "        home_runs.append(runs)\n",
    "    return home_runs\n",
    "\n",
    "profile_string = \"home_runs = calculate_runs(teams)\"\n",
    "cProfile.run(profile_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "def task(team):\n",
    "    print(team)\n",
    "for i, team in enumerate(teams):\n",
    "    thread = threading.Thread(target=task, args=(team,))\n",
    "    thread.start()\n",
    "    print(\"Started task {}\".format(i))\n",
    "\n",
    "print(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "def task(team):\n",
    "    print(team)\n",
    "def task(team):\n",
    "    time.sleep(3)\n",
    "    print(team)\n",
    "\n",
    "for i, team in enumerate(teams):\n",
    "    thread = threading.Thread(target=task, args=(team,))\n",
    "    thread.start()\n",
    "    print(\"Started task {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "def task(team):\n",
    "    print(team)\n",
    "for i in range(11):\n",
    "    team_names = teams[i*5:(i+1)*5]\n",
    "    threads = []\n",
    "    for team in team_names:\n",
    "        thread = threading.Thread(target=task, args=(team,))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    print(\"Finished batch {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def task(team):\n",
    "    print(team)\n",
    "lock = threading.Lock()\n",
    "\n",
    "def task(team):\n",
    "    lock.acquire()\n",
    "    print(team)\n",
    "    sys.stdout.flush()\n",
    "    lock.release()\n",
    "\n",
    "for i in range(11):\n",
    "    team_names = teams[i*5:(i+1)*5]\n",
    "    threads = []\n",
    "    for team in team_names:\n",
    "        thread = threading.Thread(target=task, args=(team,))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    print(\"Finished batch {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import sqlite3\n",
    "import threading\n",
    "import sys\n",
    "\n",
    "query = \"SELECT DISTINCT teamID from Teams inner join TeamsFranchises on Teams.franchID == TeamsFranchises.franchID where TeamsFranchises.active = 'Y';\"\n",
    "conn = sqlite3.connect(\"lahman2015.sqlite\", check_same_thread=False)\n",
    "cur = conn.cursor()\n",
    "teams = [row[0] for row in cur.execute(query).fetchall()]\n",
    "\n",
    "query = \"SELECT SUM(HR) FROM Batting WHERE teamId=?\"\n",
    "lock = threading.Lock()\n",
    "\n",
    "def calculate_runs(team):\n",
    "    cur = conn.cursor()\n",
    "    runs = cur.execute(query, [team]).fetchall()\n",
    "    runs = runs[0][0]\n",
    "    lock.acquire()\n",
    "    print(team)\n",
    "    print(runs)\n",
    "    sys.stdout.flush()\n",
    "    lock.release()\n",
    "    return runs\n",
    "\n",
    "threads = []\n",
    "for team in teams:\n",
    "    thread = threading.Thread(target=calculate_runs, args=(team,))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "conn = sqlite3.connect(\"lahman2015.sqlite\", check_same_thread=False)\n",
    "best = {}\n",
    "\n",
    "def best_batter():\n",
    "    pass\n",
    "\n",
    "def best_pitcher():\n",
    "    pass\n",
    "\n",
    "def best_fielder():\n",
    "    pass\n",
    "best = {}\n",
    "lock = threading.Lock()\n",
    "\n",
    "def best_batter():\n",
    "    cur = conn.cursor()\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        ((CAST(H AS FLOAT) + BB + HBP) / (AB + BB + HBP + SF)) + ((H + \"2B\" + 2*\"3B\" + 3*HR) / AB) as OBP,  \n",
    "        playerID\n",
    "    FROM Batting\n",
    "    GROUP BY Batting.playerID\n",
    "    HAVING AB > 100\n",
    "    ORDER BY OBP desc\n",
    "    LIMIT 20;\n",
    "    \"\"\"\n",
    "    players = cur.execute(query).fetchall()\n",
    "    names = [p[1] for p in players]\n",
    "    best[\"batter\"] = names\n",
    "    lock.acquire()\n",
    "    print(\"Done finding best batters.\")\n",
    "    lock.release()\n",
    "    \n",
    "    \n",
    "def best_pitcher():\n",
    "    cur = conn.cursor()\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        ((13*CAST(HR AS FLOAT) + 3*BB - 2*SO) / IPOuts) + 3.2 as FIP,  \n",
    "        playerID\n",
    "    FROM Pitching\n",
    "    GROUP BY Pitching.playerID\n",
    "    HAVING IPOuts > 100\n",
    "    ORDER BY FIP asc\n",
    "    LIMIT 20;\n",
    "    \"\"\"\n",
    "    players = cur.execute(query).fetchall()\n",
    "    names = [p[1] for p in players]\n",
    "    best[\"pitcher\"] = names\n",
    "    lock.acquire()\n",
    "    print(\"Done finding best pitchers.\")\n",
    "    lock.release()\n",
    "\n",
    "def best_fielder():\n",
    "    cur = conn.cursor()\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        (CAST(A AS FLOAT) + PO) / G as RF,  \n",
    "        playerID\n",
    "    FROM Fielding\n",
    "    GROUP BY Fielding.playerID\n",
    "    HAVING G > 100\n",
    "    ORDER BY RF desc\n",
    "    LIMIT 20;\n",
    "    \"\"\"\n",
    "    players = cur.execute(query).fetchall()\n",
    "    names = [p[1] for p in players]\n",
    "    best[\"fielder\"] = names\n",
    "    lock.acquire()\n",
    "    print(\"Done finding best fielders.\")\n",
    "    lock.release()\n",
    "\n",
    "threads = []\n",
    "for func in [best_fielder, best_batter, best_pitcher]:\n",
    "    thread = threading.Thread(target=func)\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import statistics\n",
    "def read_data():\n",
    "    with open(\"Emails.csv\") as f:\n",
    "        data = f.read()\n",
    "        \n",
    "times = []\n",
    "for i in range(100):\n",
    "    start = time.time()\n",
    "    read_data()\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "\n",
    "threaded_times = []\n",
    "for i in range(100):\n",
    "    start = time.time()\n",
    "\n",
    "    t1 = threading.Thread(target=read_data)\n",
    "    t2 = threading.Thread(target=read_data)\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "\n",
    "    for thread in [t1, t2]:\n",
    "        thread.join()\n",
    "\n",
    "    end = time.time()\n",
    "    threaded_times.append(end - start)\n",
    "\n",
    "print(statistics.median(times))\n",
    "print(statistics.median(threaded_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import time\n",
    "\n",
    "emails = pandas.read_csv(\"Emails.csv\")\n",
    "capital_letters = []\n",
    "start = time.time()\n",
    "for email in emails[\"RawText\"]:\n",
    "    capital_letters.append(len([letter for letter in email if letter.isupper()]))\n",
    "total = time.time() - start\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "capital_letters1 = []\n",
    "capital_letters2 = []\n",
    "start = time.time()\n",
    "def count_capital_letters(email):\n",
    "    return len([letter for letter in email if letter.isupper()])\n",
    "\n",
    "def count_capitals_in_emails(start, finish, capital_letters):\n",
    "    for email in emails[\"RawText\"][start:finish]:\n",
    "        capital_letters.append(count_capital_letters(email))\n",
    "\n",
    "t1 = threading.Thread(target=count_capitals_in_emails, args=(0, 3972, capital_letters1))\n",
    "t2 = threading.Thread(target=count_capitals_in_emails, args=(3972, 7946, capital_letters2))\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "for thread in [t1, t2]:\n",
    "    thread.join()\n",
    "    \n",
    "total = time.time() - start\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "capital_letters1 = []\n",
    "capital_letters2 = []\n",
    "start = time.time()\n",
    "def count_capital_letters(email):\n",
    "    return len([letter for letter in email if letter.isupper()])\n",
    "\n",
    "def count_capitals_in_emails(start, finish, capital_letters):\n",
    "    for email in emails[\"RawText\"][start:finish]:\n",
    "        capital_letters.append(count_capital_letters(email))\n",
    "\n",
    "p1 = multiprocessing.Process(target=count_capitals_in_emails, args=(0, 3972, capital_letters1))\n",
    "p2 = multiprocessing.Process(target=count_capitals_in_emails, args=(3972, 7946, capital_letters2))\n",
    "p1.start()\n",
    "p2.start()\n",
    "\n",
    "for process in [p1, p2]:\n",
    "    process.join()\n",
    "    \n",
    "total = time.time() - start\n",
    "\n",
    "print(total)\n",
    "print(capital_letters1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import multiprocessing\n",
    "start = time.time()\n",
    "def count_capital_letters(email):\n",
    "    return len([letter for letter in email if letter.isupper()])\n",
    "\n",
    "def count_capitals_in_emails(start, finish):\n",
    "    for email in emails[\"RawText\"][start:finish]:\n",
    "        count_capital_letters(email)\n",
    "\n",
    "p1 = multiprocessing.Process(target=count_capitals_in_emails, args=(0, 1986,))\n",
    "p2 = multiprocessing.Process(target=count_capitals_in_emails, args=(1986, 3972,))\n",
    "p3 = multiprocessing.Process(target=count_capitals_in_emails, args=(3972, 5958,))\n",
    "p4 = multiprocessing.Process(target=count_capitals_in_emails, args=(5958, 7946,))\n",
    "p1.start()\n",
    "p2.start()\n",
    "p3.start()\n",
    "p4.start()\n",
    "\n",
    "for process in [p1, p2, p3, p4]:\n",
    "    process.join()\n",
    "    \n",
    "total = time.time() - start\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def count_capital_letters(email):\n",
    "    return len([letter for letter in email if letter.isupper()])\n",
    "\n",
    "def count_capitals_in_emails(start, finish):\n",
    "    for email in emails[\"RawText\"][start:finish]:\n",
    "        capital_letters.append(count_capital_letters(email))\n",
    "\n",
    "start = time.time()\n",
    "p1 = multiprocessing.Process(target=count_capitals_in_emails, args=(0, 3972))\n",
    "p2 = multiprocessing.Process(target=count_capitals_in_emails, args=(3972, 7946))\n",
    "p1.start()\n",
    "p2.start()\n",
    "\n",
    "for process in [p1, p2]:\n",
    "    process.join()\n",
    "    \n",
    "total = time.time() - start\n",
    "\n",
    "print(total)\n",
    "def count_capitals_in_emails(start, finish, conn):\n",
    "    capital_letters = []\n",
    "    for email in emails[\"RawText\"][start:finish]:\n",
    "        capital_letters.append(count_capital_letters(email))\n",
    "    conn.send(capital_letters)\n",
    "    conn.close()\n",
    "\n",
    "start = time.time()\n",
    "parent_conn1, child_conn1 = multiprocessing.Pipe()\n",
    "parent_conn2, child_conn2 = multiprocessing.Pipe()\n",
    "\n",
    "p1 = multiprocessing.Process(target=count_capitals_in_emails, args=(0, 3972, child_conn1))\n",
    "p2 = multiprocessing.Process(target=count_capitals_in_emails, args=(3972, 7946, child_conn2))\n",
    "p1.start()\n",
    "p2.start()\n",
    "\n",
    "capital_letters1 = parent_conn1.recv()\n",
    "capital_letters2 = parent_conn2.recv()\n",
    "\n",
    "for process in [p1, p2]:\n",
    "    process.join()\n",
    "    \n",
    "total = time.time() - start\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "p = Pool(2)\n",
    "\n",
    "def count_capital_letters(email):\n",
    "    return len([letter for letter in email if letter.isupper()])\n",
    "\n",
    "start = time.time()\n",
    "capital_letters = p.map(count_capital_letters, emails[\"RawText\"])\n",
    "total = time.time() - start\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "nums = [1,10,20,50]\n",
    "import math\n",
    "\n",
    "pool = concurrent.futures.ProcessPoolExecutor(max_workers=10)\n",
    "roots = pool.map(math.sqrt, nums)\n",
    "roots = list(roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "\n",
    "def file_length(filename):\n",
    "    with open(filename) as f:\n",
    "        counter = 0\n",
    "        for line in f:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "results = []\n",
    "pool = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n",
    "filenames = [\"lines/{}\".format(f) for f in os.listdir(\"lines\")]\n",
    "lengths = pool.map(file_length, filenames)\n",
    "lengths = list(lengths)\n",
    "\n",
    "movie_lengths = {}\n",
    "for i in range(len(lengths)):\n",
    "    movie_lengths[filenames[i].replace(\"lines/\", \"\")] = lengths[i]\n",
    "\n",
    "most_lines = max(movie_lengths.keys(), key=(lambda k: movie_lengths[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_line(filename):\n",
    "    with open(filename) as f:\n",
    "        max_length = 0\n",
    "        longest_line = \"\"\n",
    "        for line in f:\n",
    "            line_length = len(line)\n",
    "            if line_length > max_length:\n",
    "                max_length = line_length\n",
    "                longest_line = line\n",
    "    return max_length, longest_line\n",
    "\n",
    "results = []\n",
    "pool = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n",
    "filenames = [\"lines/{}\".format(f) for f in os.listdir(\"lines\")]\n",
    "lengths = pool.map(longest_line, filenames)\n",
    "lengths = list(lengths)\n",
    "\n",
    "line_lengths = {}\n",
    "for i in range(len(lengths)):\n",
    "    line_lengths[filenames[i].replace(\"lines/\", \"\")] = lengths[i]\n",
    "\n",
    "longest_line_movie = max(line_lengths.keys(), key=(lambda k: line_lengths[k][0]))\n",
    "longest_line = line_lengths[longest_line_movie][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "def most_common_word(filename):\n",
    "    with open(filename) as f:\n",
    "        words = f.read().split(\" \")\n",
    "    from collections import Counter\n",
    "    count = Counter(words)\n",
    "\n",
    "    return count.most_common()[0][0]\n",
    "\n",
    "results = []\n",
    "start = time.time()\n",
    "pool = concurrent.futures.ProcessPoolExecutor(max_workers=2)\n",
    "filenames = [\"lines/{}\".format(f) for f in os.listdir(\"lines\")]\n",
    "words = pool.map(most_common_word, filenames)\n",
    "words = list(words)\n",
    "\n",
    "end = time.time()\n",
    "common_words = {}\n",
    "for i in range(len(lengths)):\n",
    "    common_words[filenames[i].replace(\"lines/\", \"\")] = words[i]\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "import os\n",
    "\n",
    "def most_common_word(filename):\n",
    "    # Fill in the function here\n",
    "    return \"\"\n",
    "\n",
    "results = []\n",
    "pool = concurrent.futures.ProcessPoolExecutor(max_workers=5)\n",
    "filenames = [\"lines/{}\".format(f) for f in os.listdir(\"lines\")]\n",
    "words = pool.map(most_common_word, filenames)\n",
    "words = list(words)\n",
    "\n",
    "common_words = {}\n",
    "for i in range(len(lengths)):\n",
    "    common_words[filenames[i].replace(\"lines/\", \"\")] = words[i]\n",
    "import os\n",
    "\n",
    "def most_common_word(filename):\n",
    "    with open(filename) as f:\n",
    "        data = f.read().lower()\n",
    "    import re\n",
    "    data = re.sub(\"\\W+\", \" \", data)\n",
    "    words = data.split(\" \")\n",
    "    words = [w for w in words if len(w) >= 5]\n",
    "    from collections import Counter\n",
    "    count = Counter(words)\n",
    "    return count.most_common()[0][0]\n",
    "\n",
    "results = []\n",
    "pool = concurrent.futures.ProcessPoolExecutor(max_workers=2)\n",
    "filenames = [\"lines/{}\".format(f) for f in os.listdir(\"lines\")]\n",
    "words = pool.map(most_common_word, filenames)\n",
    "words = list(words)\n",
    "\n",
    "common_words = {}\n",
    "for i in range(len(lengths)):\n",
    "    common_words[filenames[i].replace(\"lines/\", \"\")] = words[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
